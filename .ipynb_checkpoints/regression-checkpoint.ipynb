{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03abdb87-b228-49b6-be0c-4f347fc29407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from requests import get\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "from pyproj import CRS\n",
    "from shapely.geometry import Point\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5d3685-f508-4e79-80f8-f95f16b8558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_data(src:str, dest:str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL `src` and saves it to the `dest` directory.\n",
    "    If the file already exists and is of sufficient size, skips the download.\n",
    "    \n",
    "    Args:\n",
    "        src (str): The source URL of the file to be downloaded.\n",
    "        dest (str): The destination directory where the file will be saved.\n",
    "        \n",
    "    Returns:\n",
    "        str: The full path to the downloaded (or existing) file.\n",
    "    \"\"\"    \n",
    "    url = urlparse(src) # We assume that this is some kind of valid URL \n",
    "    fn  = os.path.split(url.path)[-1] # Extract the filename\n",
    "    dfn = os.path.join(dest,fn) # Destination filename\n",
    "    \n",
    "    if not os.path.isfile(dfn) or os.path.getsize(dfn) < 250:\n",
    "        \n",
    "        print(f\"{dfn} not found, downloading!\")\n",
    "\n",
    "        path = os.path.split(dest)\n",
    "        \n",
    "        if len(path) >= 1 and path[0] != '':\n",
    "            os.makedirs(os.path.join(*path), exist_ok=True)\n",
    "            \n",
    "        with open(dfn, \"wb\") as file:\n",
    "            response = get(src)\n",
    "            file.write(response.content)\n",
    "            \n",
    "        print(\"\\tDone downloading...\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Found {dfn} locally!\")\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c9a2a1-3d21-48e3-b891-9f3461867fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing data,choose the data of 2024\n",
    "ymd  = '20240614'\n",
    "city = 'London'\n",
    "host = 'https://orca.casa.ucl.ac.uk'\n",
    "list_url  = f'{host}/~jreades/data/{ymd}-{city}-listings.parquet'\n",
    "transport_url = 'https://github.com/Chocolateyz/Chocolate/raw/refs/heads/main/Transport_Classification_of_Londoners_(TCoL)/Transport_Classification_of_Londoners_(TCoL).shp'\n",
    "inequity_url = 'https://github.com/Chocolateyz/Chocolate/raw/refs/heads/main/Borough_pages_data_UPDATE_9_2024_HACTAR(1).csv'\n",
    "pop_url = 'https://github.com/Chocolateyz/Chocolate/raw/refs/heads/main/population%20by%20borough.xlsx'\n",
    "sta_url = 'https://github.com/Chocolateyz/Chocolate/raw/refs/heads/main/London%20stations/London%20stations.shp'\n",
    "line_url = 'https://github.com/Chocolateyz/Chocolate/raw/refs/heads/main/London%20Train%20Lines/London%20Train%20Lines.shp'\n",
    "borough_url = 'https://github.com/Chocolateyz/Chocolate/raw/refs/heads/main/Boroughs.gpkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6162ed-f2d6-4168-a05b-1023ba543c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data/raw/20240614-London-listings.parquet locally!\n",
      "Found data/raw/Borough_pages_data_UPDATE_9_2024_HACTAR(1).csv locally!\n",
      "Found data/raw/population%20by%20borough.xlsx locally!\n",
      "Found data/raw/Boroughs.gpkg locally!\n"
     ]
    }
   ],
   "source": [
    "# the data frame of datas\n",
    "listing_df = pd.read_parquet(cache_data(list_url, os.path.join('data','raw')))\n",
    "# inequity data\n",
    "inequity = pd.read_csv(cache_data(inequity_url, os.path.join('data','raw')),skiprows=11, header=0)\n",
    "# population data\n",
    "pop = pd.read_excel(cache_data(pop_url, os.path.join('data','raw')))\n",
    "# borough outline\n",
    "borough = gpd.read_file(cache_data(borough_url, os.path.join('data','raw')))\n",
    "\n",
    "# station point data\n",
    "station = gpd.read_file(sta_url)\n",
    "# train line data\n",
    "line = gpd.read_file(line_url)\n",
    "# transport data, spatial unit is OA(output area)\n",
    "transport = gpd.read_file(transport_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e76fcca-f789-46dc-bdfa-1dcb12bedbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing data columns: ['id', 'listing_url', 'last_scraped', 'name', 'description', 'host_id', 'host_name', 'host_since', 'host_location', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'host_verifications', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'availability_365', 'number_of_reviews', 'first_review', 'last_review', 'review_scores_rating', 'reviews_per_month']\n",
      "Data frame is 85,127 x 31\n",
      "transport data columns: ['OBJECTID', 'TCOL_SEGME', 'TCOL_SEG_1', 'OA_POPULAT', 'SEGMENT_PO', 'LOAC_SUPER', 'LOAC_GROUP', 'LOAC_SUB_G', 'LOCAL_AUTH', 'PROPENSITY', 'PROPENSI_1', 'PROPENSI_2', 'PROPENSI_3', 'CAR_DRIVER', 'CAR_PASSEN', 'BUS_TRIPS_', 'TRAIN_TRIP', 'RIVER_TRIP', 'TRAM_TRIPS', 'TUBE_TRIPS', 'BICYCLE_TR', 'HIRE_BIKE_', 'DLR_TRIPS_', 'POWERED_TW', 'TAXI_PHV_T', 'WALK_TRIPS', 'CAR_AVAILA', 'AVERAGE_IN', 'STUDENT', 'NO_CHILDRE', 'NO_CHILD_1', 'NO_CHILD_2', 'WITH_CHILD', 'WITH_CHI_1', 'WITH_CHI_2', 'RETIRED', 'REDUCED_CA', 'INCREASED_', 'INCREASE_1', 'INCREASE_2', 'INCREASE_3', 'INCREASE_4', 'OA_ID', 'DATA_FILE', 'USER_GUIDE', 'METADATA', 'Shape__Are', 'Shape__Len', 'geometry']\n",
      "inequity data columns: ['Area', 'Poverty rate', 'Child poverty rate (AHC)', 'Income deprivation (relative to London overall)', '80:20 ratio of earnings', 'Main homelessness duty owed per 1,000 households', 'Households in temporary accommodation per 1,000', 'Repossessions per 1,000 households', 'Median rent as a percentage of median pay', 'Average net affordable, social and discounted housing completions', 'People seen sleeping rough by outreach', \"Proportion of borough residents' jobs that are low paid\", 'Unemployment rate', 'Unemployment rate 1 year change', 'Out-of-work benefits', '19 year olds without level 3 qualifications', 'Infant mortality rate per 1,000 live births', 'Deaths of <75 year olds per 100,000', 'Percentage of pupils who achieved grade 9-4', 'Proportion with no qualifications']\n",
      "population data columns: ['GSS_CODE', 'NAME', 'POPULATION']\n",
      "station data columns: ['Name', 'Link', 'Zone', 'Postcode', 'geometry']\n",
      "line data columns: ['Name', 'geometry']\n",
      "borough data columns: ['NAME', 'GSS_CODE', 'HECTARES', 'NONLD_AREA', 'ONS_INNER', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# this is the columns list, if you want to focus on a certain topic, choose some of them\n",
    "# listing data\n",
    "columns_list = listing_df.columns.to_list()\n",
    "print(f'listing data columns: {columns_list}')\n",
    "print(f\"Data frame is {listing_df.shape[0]:,} x {listing_df.shape[1]}\")\n",
    "# transport data\n",
    "columns_transport = transport.columns.to_list()\n",
    "print(f'transport data columns: {columns_transport}')\n",
    "# inequity data\n",
    "columns_inequity = inequity.columns.to_list()\n",
    "print(f'inequity data columns: {columns_inequity}')\n",
    "# population data\n",
    "columns_pop = pop.columns.to_list()\n",
    "print(f'population data columns: {columns_pop}')\n",
    "# station data\n",
    "columns_station = station.columns.to_list()\n",
    "print(f'station data columns: {columns_station}')\n",
    "# line data\n",
    "columns_line = line.columns.to_list()\n",
    "print(f'line data columns: {columns_line}')\n",
    "# borough outline\n",
    "columns_borough = borough.columns.to_list()\n",
    "print(f'borough data columns: {columns_borough}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef501c1b-7ca7-4f83-82bc-7c7be29fb58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                name  \\\n",
      "0  Rental unit in Earlsfield · ★4.57 · 1 bedroom ...   \n",
      "1  Rental unit in Hammersmith · ★4.82 · 2 bedroom...   \n",
      "2  Rental unit in Islington · ★4.80 · 1 bedroom ·...   \n",
      "3  Rental unit in London · ★4.80 · 1 bedroom · 1 ...   \n",
      "4  Condo in London · ★4.62 · 1 bedroom · 1 bed · ...   \n",
      "\n",
      "                        geometry  \n",
      "0  POINT (526071.772 173004.347)  \n",
      "1  POINT (523852.404 179394.541)  \n",
      "2  POINT (530899.228 187213.456)  \n",
      "3  POINT (527282.901 178129.531)  \n",
      "4  POINT (539991.817 177170.364)  \n"
     ]
    }
   ],
   "source": [
    "# Convert df to gdf\n",
    "listing_df['geometry'] = listing_df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "listing_gdf = gpd.GeoDataFrame(listing_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "listing_gdf = listing_gdf.to_crs(epsg=27700)\n",
    "\n",
    "# select columns\n",
    "geo_list = listing_gdf[['name','geometry']]\n",
    "\n",
    "# check result\n",
    "print(geo_list.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28536fce-1224-4f94-8389-aa1f3f39dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of short-term rental data points contained in each borough\n",
    "joined_gdf = gpd.sjoin(geo_list, borough, how=\"inner\", predicate=\"within\")\n",
    "point_counts = joined_gdf.groupby('GSS_CODE').size().reset_index(name='point_count')\n",
    "#borough.set_index('GSS_CODE', inplace=True)\n",
    "#borough.reset_index(inplace=True)\n",
    "\n",
    "# add point count into borough\n",
    "borough_list = borough.merge(point_counts, on='GSS_CODE', how='left')\n",
    "\n",
    "# set zero if there is no point\n",
    "#borough_list['point_count'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fedc8fc-90d5-43f3-bbe7-f9ad1480c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify the coordinate system\n",
    "station = station.to_crs(epsg=27700)\n",
    "line = line.to_crs(epsg=27700)\n",
    "# Count the number of underground stations in each borough and join the result to the borough data\n",
    "station_join = gpd.sjoin(station, borough, how=\"inner\", predicate=\"within\")\n",
    "station_counts = station_join.groupby('GSS_CODE').size().reset_index(name='station_count')\n",
    "regression1 = borough_list.merge(station_counts,on='GSS_CODE', how='left')\n",
    "\n",
    "# Calculate the track length in each borough and join the result to the borough data\n",
    "line_join = gpd.sjoin(line, borough, how=\"inner\", predicate=\"within\")\n",
    "line_join['line_length'] = line_join.geometry.length\n",
    "lengths_per_borough = line_join.groupby('GSS_CODE')['line_length'].sum().reset_index()\n",
    "regression2 = regression1.merge(lengths_per_borough,on='GSS_CODE', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60569118-2782-47b7-b0b2-1890049ec8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the format of borough names\n",
    "inequity['Area'] = inequity['Area'].str.replace('&','and')\n",
    "# Join the population and inequality index data together\n",
    "pop_inequity = pd.merge(pop, inequity, left_on='NAME', right_on='Area', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4019b759-f942-4515-82f3-d53ad8620018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all of the data\n",
    "regression3 = regression2.merge(pop_inequity,on='GSS_CODE', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209f8479-404b-42fc-8cb6-1d1287dae89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAME_x', 'GSS_CODE', 'HECTARES', 'NONLD_AREA', 'ONS_INNER', 'geometry', 'point_count', 'station_count', 'line_length', 'NAME_y', 'POPULATION', 'Area', 'Poverty rate', 'Child poverty rate (AHC)', 'Income deprivation (relative to London overall)', '80:20 ratio of earnings', 'Main homelessness duty owed per 1,000 households', 'Households in temporary accommodation per 1,000', 'Repossessions per 1,000 households', 'Median rent as a percentage of median pay', 'Average net affordable, social and discounted housing completions', 'People seen sleeping rough by outreach', \"Proportion of borough residents' jobs that are low paid\", 'Unemployment rate', 'Unemployment rate 1 year change', 'Out-of-work benefits', '19 year olds without level 3 qualifications', 'Infant mortality rate per 1,000 live births', 'Deaths of <75 year olds per 100,000', 'Percentage of pupils who achieved grade 9-4', 'Proportion with no qualifications']\n"
     ]
    }
   ],
   "source": [
    "# check the columns\n",
    "regression_Column = regression3.columns.to_list()\n",
    "print(regression_Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "119b2de1-d29e-4f79-b157-ea527af840d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/lib/python3.11/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/lib/python3.11/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/lib/python3.11/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "# caculate densities\n",
    "regression4 = regression3[['GSS_CODE','Area','HECTARES','Poverty rate','station_count','line_length','POPULATION','point_count','80:20 ratio of earnings','geometry']]\n",
    "regression4['population_density'] = regression3['POPULATION'] / regression3['HECTARES']\n",
    "regression4['airbnb_density'] = regression3['point_count'] / regression3['HECTARES']\n",
    "regression4['station_density'] = regression3['station_count'] / regression3['HECTARES']\n",
    "regression4['line_density'] = regression3['line_length'] / regression3['HECTARES']\n",
    "# select columns for regression,remove NA\n",
    "regression_all = regression4.drop(columns=['POPULATION', 'HECTARES','point_count','station_count','line_length']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1d0517-ba03-4490-8ba0-40b5a89ca200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected: 3\n",
      "Cumulative explained variance ratio: 0.97\n",
      "[[-1.42893067e+00 -2.29305204e-01  3.18513056e-02]\n",
      " [-5.61973274e-01 -4.62151413e-01  8.29208447e-01]\n",
      " [-2.03229615e+00 -7.37397025e-01  6.97413192e-01]\n",
      " [-1.05664842e+00  8.10786179e-01 -1.35541269e-01]\n",
      " [ 1.93431840e-01  5.07251566e-01  5.99921275e-01]\n",
      " [-2.62671533e+00  1.67009874e-02 -1.51146590e-01]\n",
      " [-1.65669596e+00  4.33141237e-01  5.14906320e-01]\n",
      " [-1.67495575e+00  7.34032511e-02 -5.69205753e-01]\n",
      " [ 6.84482870e-01  3.48158101e-01  2.95646840e-01]\n",
      " [-1.47235807e+00  5.50176353e-01 -2.49146044e-01]\n",
      " [ 1.85943108e+00 -1.17528022e+00  5.58499179e-01]\n",
      " [ 1.04046946e+00 -1.42365460e+00  5.86238358e-01]\n",
      " [ 1.02339031e+00 -1.71524037e-01  7.24207640e-01]\n",
      " [-8.63193448e-01  2.76735635e-01  9.38911928e-03]\n",
      " [-1.94075651e+00 -1.69216626e-01 -4.55720800e-05]\n",
      " [-8.83494822e-01  1.02561959e+00  2.64565372e-01]\n",
      " [-7.72413198e-01 -2.54751520e-01 -3.27920785e-01]\n",
      " [-1.20787947e+00  1.06589573e+00 -6.78361360e-01]\n",
      " [-1.14598973e+00 -2.54518868e-01  4.19167947e-01]\n",
      " [-4.93188848e-01 -1.51950929e+00  6.54137904e-01]\n",
      " [ 5.71937404e-02  4.28879890e-01 -4.38267329e-01]\n",
      " [ 3.89232944e-01 -1.10314216e+00 -1.21408896e+00]\n",
      " [ 3.67656558e+00  8.61809647e-01  8.40795706e-01]\n",
      " [ 3.54403209e+00  1.01260264e+00  7.86487513e-01]\n",
      " [ 3.28325425e+00  9.26921609e-01 -9.41799730e-01]\n",
      " [ 2.21770363e+00 -1.89705308e+00 -9.30260365e-01]\n",
      " [ 1.78074600e+00 -5.91918643e-01 -9.34992584e-01]\n",
      " [-1.06636293e-01 -4.94987903e-01 -6.03322614e-01]\n",
      " [ 1.11616981e+00  1.29807749e+00 -5.05746052e-01]\n",
      " [-9.41977663e-01  8.48250689e-01 -1.32591111e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Use short-term rental point data as the dependent variable, excluding the impacts of economy, population, and transportation\n",
    "X = regression_all.drop(columns=['geometry','airbnb_density','80:20 ratio of earnings','GSS_CODE','Area'])  # independent variables\n",
    "Y = regression_all['airbnb_density'] \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Principal Component Analysis (PCA) to avoid multicollinearity.\n",
    "pca = PCA(n_components=0.95)  \n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "# check result\n",
    "print(f\"Number of components selected: {pca.n_components_}\")\n",
    "print(f\"Cumulative explained variance ratio: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "print(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2db26d-97e7-45b8-9691-93dff676e443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8364144295300542\n",
      "0.8182790588218255\n",
      "0     0.062471\n",
      "1    -0.270448\n",
      "2     0.469750\n",
      "3    -0.034502\n",
      "4    -0.466623\n",
      "5     0.659774\n",
      "6     0.376119\n",
      "7    -0.029200\n",
      "8    -0.682929\n",
      "9     0.182416\n",
      "10   -0.491682\n",
      "11   -0.072233\n",
      "12   -0.735423\n",
      "13   -0.040035\n",
      "14    0.279505\n",
      "15   -0.088038\n",
      "16   -0.251470\n",
      "17   -0.159313\n",
      "18   -0.087337\n",
      "20   -0.342050\n",
      "21    0.090394\n",
      "22    0.286830\n",
      "24    1.497735\n",
      "25   -0.259511\n",
      "26   -0.134570\n",
      "27   -0.021253\n",
      "28    0.563975\n",
      "29   -0.459844\n",
      "30   -1.133302\n",
      "31   -0.176425\n",
      "Name: airbnb_density, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets for linear fitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# regression analysis\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "r2_train = model.score(X_train, Y_train)\n",
    "r2_test = model.score(X_test, Y_test)\n",
    "Y_pred_all = model.predict(X_pca)\n",
    "\n",
    "# Calculate the residuals for all boroughs\n",
    "residuals_all = Y - Y_pred_all\n",
    "print(r2_train)\n",
    "print(r2_test)\n",
    "print(residuals_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2297fc11-43b0-4970-b5f1-b645b6a123a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the coordinates (longitude, latitude) of the centroids of the geometries\n",
    "coords = regression_all.geometry.centroid.apply(lambda x: (x.x, x.y)).to_list()\n",
    "# Convert the list of coordinates into a NumPy array\n",
    "coords = np.array(coords)\n",
    "# Use the '80:20 ratio of earnings' as an indicator of inequality.\n",
    "# Extract the values of the '80:20 ratio of earnings' column as the dependent variable\n",
    "real_y = regression_all['80:20 ratio of earnings'].values \n",
    "# Choose residuals from the previous regression model as independent variable\n",
    "residuals = residuals_all.values \n",
    "\n",
    "\n",
    "# Scale the residuals to standardize them\n",
    "scaler = StandardScaler()\n",
    "residuals_scaled = scaler.fit_transform(residuals.reshape(-1, 1))\n",
    "# Scale the real_y values to standardize them\n",
    "real_y_scaled = scaler.fit_transform(real_y.reshape(-1, 1))\n",
    "# Add a constant term\n",
    "X_loc = np.hstack([np.ones((residuals_scaled.shape[0], 1)), residuals_scaled])\n",
    "# Select the optimal bandwidth\n",
    "#selector = Sel_BW(coords=coords, y=real_y_scaled, X_loc=X_loc)\n",
    "#bandwidth = selector.search(criterion='AICc',bw_min=5, bw_max=15)\n",
    "bandwidth = 13\n",
    "#print(f\"Optimal bandwidth: {bandwidth}\")\n",
    "regression_data = np.column_stack([residuals_scaled, real_y_scaled])\n",
    "\n",
    "# Fit a Geographically Weighted Regression (GWR) model\n",
    "gwr_model = GWR(coords = coords, y=real_y_scaled, X = X_loc, bw = bandwidth)\n",
    "gwr_results = gwr_model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8c22c-be12-4e8a-8e73-7c96d4ca8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取回归系数\n",
    "gwr_params = gwr_results.params  # 每个坐标点的回归系数\n",
    "\n",
    "# 创建 DataFrame 查看结果\n",
    "coef_df = pd.DataFrame(gwr_params, columns=['Intercept', 'Accommodates_Coef', 'RoomType_Coef'])\n",
    "# Print a summary of the GWR model results\n",
    "print(gwr_results.summary())\n",
    "print(gwr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e07e2-0f97-4cf1-9512-54c9689fc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define R² and Adjusted R² values for global regression and GWR regression\n",
    "global_r2 = 0.044\n",
    "global_adj_r2 = 0.010\n",
    "gwr_r2 = 0.523\n",
    "gwr_adj_r2 = 0.249\n",
    "\n",
    "# Create a dictionary to hold the regression results\n",
    "results_dict = {\n",
    "    \"Model Type\": [\"Global Regression\", \"GWR Regression\"],\n",
    "    \"R²\": [global_r2, gwr_r2],\n",
    "    \"Adjusted R²\": [global_adj_r2, gwr_adj_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to DataFrame\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Create a figure and axis for the table visualization\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=results_df.values, colLabels=results_df.columns, loc='center')\n",
    "\n",
    "# Customize the appearance of the table cells\n",
    "for (i, j), cell in table.get_celld().items():\n",
    "    cell.set_text_props(horizontalalignment='center', verticalalignment='center', fontsize=12,fontname='Source Serif 4')\n",
    "    if i == 0:\n",
    "        cell.set_fontsize(14)\n",
    "        cell.set_text_props(weight='bold')  \n",
    "        cell.set_facecolor((176/255, 224/255, 230/255, 0.5)) \n",
    "    else:\n",
    "        cell.set_facecolor((0.878, 1, 1,0.2))\n",
    "    cell.set_height(0.15)\n",
    "\n",
    "# Add title, adjust its position\n",
    "plt.title(\"R² Comparison Between Global Regression and GWR\", fontsize=14,fontname='Source Serif 4',y=0.8)\n",
    "# Display the table\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e15ae-1f4e-40d7-b8fd-8d2a9b7583ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# representing the local R square\n",
    "local_r2_values = gwr_results.localR2\n",
    "print(local_r2_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610db09-b05e-4a1f-afcd-4d4499622da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the obtained local R² values back to the borough data\n",
    "regression_all['local_R2'] = local_r2_values\n",
    "r2_df = regression_all[['GSS_CODE','local_R2']]\n",
    "borough_r2 = borough.merge(r2_df,on = 'GSS_CODE', how='left')\n",
    "print(borough_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a0f2c-8ae1-4806-9f3a-cd4f6a5f857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the coordinate reference system\n",
    "borough_r2 = borough_r2.to_crs(epsg=4326)\n",
    "# Replace 'NA' values with NaN\n",
    "borough_r2['local_R2'] = borough_r2['local_R2'].replace('NA', np.nan)\n",
    "# Create a figure and axis for plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot the borough data, using the 'local_R2' column to color the map.\n",
    "borough_r2.plot(column='local_R2', cmap='YlGnBu', legend=True,\n",
    "                missing_kwds={'color': 'gray', 'label': 'Missing values'},\n",
    "                legend_kwds={'label': \"Local R² Value\", 'orientation': \"horizontal\"},\n",
    "                ax=ax)\n",
    "borough_r2.boundary.plot(ax=ax, edgecolor='black', linewidth=0.3,alpha = 0.6)\n",
    "# Remove grid lines\n",
    "ax.grid(False)\n",
    "# Add title and label\n",
    "plt.title('Map of Local R² by Region',fontsize=16)\n",
    "plt.xlabel('Longitude',fontsize=14)\n",
    "plt.ylabel('Latitude',fontsize=14)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383a0ec-8433-4f77-85cf-0ed9de3dc793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
